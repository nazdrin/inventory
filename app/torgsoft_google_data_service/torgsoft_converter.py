# app/torgsoft_google/torgsoft_converter.py

import asyncio
import json
import logging
import math
import os
import tempfile
from typing import Any, Dict, List, Optional

import pandas as pd

# üëâ –ó–∞–º–µ–Ω–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—É—Ç—å –≤ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ, –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
try:
    from app.services.database_service import process_database_service  # type: ignore
except Exception:
    try:
        from app.services.stock_scheduler_service import process_database_service  # type: ignore
    except Exception:
        process_database_service = None  # –±—É–¥–µ—Ç –ª–æ–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞ –±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# =========================
# –í–ù–£–¢–†–ï–ù–ù–ò–ï –£–¢–ò–õ–ò–¢–´
# =========================

UA_TO_STD_MAP = {
    # –û–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–¥–∞:
    "–∫–æ–¥ —Ñ–æ—Ç–æ": "code",

    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
    "–Ω–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É": "name",
    "–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—É": "name",

    # –®—Ç—Ä–∏—Ö–∫–æ–¥
    "—à—Ç—Ä–∏—Ö-–∫–æ–¥": "barcode",
    "—à—Ç—Ä–∏—Ö–∫–æ–¥": "barcode",

    # –¶–µ–Ω—ã (–¥–ª—è —Å—Ç–æ–∫–∞)
    "—Ü—ñ–Ω–∞ —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞": "price",
    "—Ü–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è": "price",
    "—Ü—ñ–Ω–∞ –∑—ñ –∑–Ω–∏–∂–∫–æ—é": "price_reserve",
    "—Ü–µ–Ω–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π": "price_reserve",

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ (–¥–ª—è —Å—Ç–æ–∫–∞)
    "–∫—ñ–ª—å–∫—ñ—Å—Ç—å": "qty",
    "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ": "qty",

    # –ë–æ–ª—å—à–µ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º "‚Ññ" –∫–∞–∫ –∫–æ–¥ ‚Äî –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–æ
    # "‚Ññ": "code",
    # "–Ω–æ–º–µ—Ä": "code",
    # "‚Ññ –∑/–ø": "code",
}

DEFAULT_PRODUCER = "N/A"
DEFAULT_VAT = 20.0


def _get_temp_dir() -> str:
    tmp = os.getenv("TEMP_DIR", tempfile.gettempdir())
    os.makedirs(tmp, exist_ok=True)
    return tmp


def _norm(s: Any) -> str:
    return str(s).strip().lower() if s is not None else ""


def _coerce_str(v: Any) -> str:
    if v is None or (isinstance(v, float) and math.isnan(v)):
        return ""
    s = str(v).strip()
    # –°–æ—Ö—Ä–∞–Ω–∏–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏ –¥–ª—è —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–≤
    return s


def _coerce_float(v: Any, default: float = 0.0) -> float:
    try:
        if v is None or (isinstance(v, float) and math.isnan(v)):
            return default
        s = str(v).replace(",", ".").strip()
        return float(s) if s else default
    except Exception:
        return default


def _coerce_int_nonneg(v: Any, default: int = 0) -> int:
    try:
        if v is None or (isinstance(v, float) and math.isnan(v)):
            return default
        if isinstance(v, str) and not v.strip():
            return default
        val = int(float(str(v).replace(",", ".").strip()))
        return max(val, 0)
    except Exception:
        return default


def _read_table(file_path: str) -> pd.DataFrame:
    """
    –ß–∏—Ç–∞–µ—Ç Excel/CSV –∏ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.
    –†–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ 1‚Äì10 —Å—Ç—Ä–æ–∫–µ (–≤–∫–ª—é—á–∞—è 3-—é —Å—Ç—Ä–æ–∫—É).
    """
    def _normalize_cols(cols):
        out = []
        for c in cols:
            s = _norm(c)
            out.append(" ".join(s.split()))
        return out

    def _detect_header_row(df_probe: pd.DataFrame) -> int:
        # –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —É–∑–Ω–∞—ë–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header_hints = [
            "–∫–æ–¥ —Ñ–æ—Ç–æ",           # üëà –¥–æ–±–∞–≤–∏–ª–∏
            "–Ω–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É", "–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—É",
            "—à—Ç—Ä–∏—Ö-–∫–æ–¥", "—à—Ç—Ä–∏—Ö–∫–æ–¥",
            "–∫—ñ–ª—å–∫—ñ—Å—Ç—å", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "—Ü—ñ–Ω–∞ —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞", "—Ü–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è",
            "—Ü—ñ–Ω–∞ –∑—ñ –∑–Ω–∏–∂–∫–æ—é", "—Ü–µ–Ω–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π",
            # (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å "‚Ññ" –≤ —Ö–∏–Ω—Ç–∞—Ö –∫–∞–∫ –≤—Ç–æ—Ä–∏—á–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞,
            # —ç—Ç–æ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è code)
            "‚Ññ", "–Ω–æ–º–µ—Ä", "‚Ññ –∑/–ø",
        ]
        max_rows_to_check = min(len(df_probe), 10)
        for i in range(max_rows_to_check):
            row_vals = _normalize_cols(list(df_probe.iloc[i].values))
            joined = " ".join(row_vals)
            # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤ —Å—Ç—Ä–æ–∫–µ –¥–æ–ª–∂–Ω—ã –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è —Ö–æ—Ç—è –±—ã 2 –ø–æ–¥—Å–∫–∞–∑–∫–∏
            hits = sum(1 for h in header_hints if h in joined)
            if hits >= 2:
                return i
        # –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
        return 0

    # --- —á–∏—Ç–∞–µ–º "—á–µ—Ä–Ω–æ–≤–∏–∫" –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —à–∞–ø–∫–∏
    if file_path.lower().endswith((".xlsx", ".xls")):
        probe = pd.read_excel(file_path, header=None, nrows=10, dtype=object)
        header_row = _detect_header_row(probe)
        df = pd.read_excel(file_path, header=header_row, dtype=object)
    elif file_path.lower().endswith(".csv"):
        try:
            probe = pd.read_csv(file_path, header=None, nrows=10, dtype=object)
        except UnicodeDecodeError:
            probe = pd.read_csv(file_path, header=None, nrows=10, dtype=object, encoding="cp1251")
        header_row = _detect_header_row(probe)
        try:
            df = pd.read_csv(file_path, header=header_row, dtype=object)
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, header=header_row, dtype=object, encoding="cp1251")
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
    df.columns = [" ".join(_norm(c).split()) for c in df.columns]

    # –ª–æ–≥ –¥–ª—è –¥–µ–±–∞–≥–∞ ‚Äî —É–≤–∏–¥–∏–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏—Å—å
    logging.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: %s", list(df.columns))
    return df



def _map_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç–æ–ª–±—Ü—ã –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –∏–º–µ–Ω–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ UA_TO_STD_MAP.
    –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å.
    """
    rename_map: Dict[str, str] = {}
    for col in df.columns:
        std = UA_TO_STD_MAP.get(col, None)
        if std:
            rename_map[col] = std
    if rename_map:
        df = df.rename(columns=rename_map)
    return df


def _save_json_temp(data: List[Dict[str, Any]], prefix: str) -> str:
    out_path = os.path.join(_get_temp_dir(), f"{prefix}_torgsoft_{next(tempfile._get_candidate_names())}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return out_path


async def _send_downstream(data: List[Dict[str, Any]], data_type: str, enterprise_code: str) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–ª—å—à–µ –ø–æ –ø—Ä–æ–µ–∫—Ç—É:
    - –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω process_database_service -> –≤—ã–∑—ã–≤–∞–µ–º –µ–≥–æ.
    - –ò–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –ø–∏—à–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ –ª–æ–≥.
    """
    if process_database_service is None:
        path = _save_json_temp(data, prefix=data_type)
        logging.warning(
            "process_database_service –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π JSON: %s", path
        )
        return

    # –°–µ—Ä–≤–∏—Å –æ–∂–∏–¥–∞–µ—Ç –ø—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É
    path = _save_json_temp(data, prefix=data_type)
    logging.info("–û—Ç–ø—Ä–∞–≤–∫–∞ %s –¥–∞–Ω–Ω—ã—Ö –≤ process_database_service: %s", data_type, path)
    try:
        if asyncio.iscoroutinefunction(process_database_service):
            await process_database_service(path, data_type, enterprise_code)
        else:
            # –ù–∞ —Å–ª—É—á–∞–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, process_database_service, path, data_type, enterprise_code)
    finally:
        try:
            os.remove(path)
        except Exception:
            pass


# =========================
# –ü–£–ë–õ–ò–ß–ù–´–ï –§–£–ù–ö–¶–ò–ò –ö–û–ù–í–ï–†–¢–ê–¶–ò–ò
# =========================

async def process_torgsoft_catalog(
    enterprise_code: str,
    file_path: str,
    file_type: str = "catalog",
) -> None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∞ ¬´–°—Ç–∞–Ω —Å–∫–ª–∞–¥¬ª.
    –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π:
      code = ¬´‚Ññ¬ª
      name = ¬´–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É¬ª
      barcode = ¬´–®—Ç—Ä–∏—Ö-–∫–æ–¥¬ª
      producer = 'N/A'
      vat = 20
    (–¶–µ–Ω—ã/–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –º–æ–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å ‚Äî —ç—Ç–æ —á–∞—Å—Ç—å —Å—Ç–æ–∫–∞)
    """
    df = _read_table(file_path)
    df = _map_columns(df)

    # –ü—Ä–æ–≤–µ—Ä–∏–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞ –ø–æ–ª—è
    missing = []
    for req in ["code", "name"]:
        if req not in df.columns:
            missing.append(req)
    if missing:
        raise ValueError(f"–í –∫–∞—Ç–∞–ª–æ–≥–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing)}")

    converted: List[Dict[str, Any]] = []
    for _, row in df.iterrows():
        item_code = _coerce_str(row.get("code"))
        item_name = _coerce_str(row.get("name"))

        if not item_code or not item_name:
            # –ø—Ä–æ–ø—É—Å—Ç–∏–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            continue

        converted.append(
            {
                "code": item_code,
                "name": item_name,
                "producer": DEFAULT_PRODUCER,
                "vat": DEFAULT_VAT,
                "barcode": _coerce_str(row.get("barcode")),
            }
        )

    logging.info("–ö–∞—Ç–∞–ª–æ–≥ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: %s –ø–æ–∑–∏—Ü–∏–π", len(converted))
    await _send_downstream(converted, data_type="catalog", enterprise_code=enterprise_code)


async def process_torgsoft_stock(
    enterprise_code: str,
    file_path: str,
    file_type: str = "stock",
    *,
    branch: str,
    single_store: Optional[bool] = None,
    store_serial: Optional[str] = None,
) -> None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç–æ–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∞ ¬´–°—Ç–∞–Ω —Å–∫–ª–∞–¥¬ª.
    –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª—ñ–≤:
      code = ¬´‚Ññ¬ª
      barcode = ¬´–®—Ç—Ä–∏—Ö-–∫–æ–¥¬ª
      price = ¬´–¶—ñ–Ω–∞ —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞¬ª
      price_reserve = ¬´–¶—ñ–Ω–∞ –∑—ñ –∑–Ω–∏–∂–∫–æ—é¬ª
      qty = ¬´–ö—ñ–ª—å–∫—ñ—Å—Ç—å¬ª (–Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è)
      branch = –ø—Ä–∏—Ö–æ–¥–∏—Ç –∏–∑ MappingBranch –ø–æ –∫–æ–¥—É –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è
    –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
      - –ï—Å–ª–∏ price_reserve –ø—É—Å—Ç–æ–π -> –∏—Å–ø–æ–ª—å–∑—É–µ–º price –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç? –ù–ï–¢. –ú—ã –ø–µ—Ä–µ–¥–∞—ë–º –æ–±–∞ –ø–æ–ª—è –∫–∞–∫ –µ—Å—Ç—å.
      - qty < 0 –ø—Ä–∏–≤–æ–¥–∏–º –∫ 0.
    """
    df = _read_table(file_path)
    df = _map_columns(df)

    missing = []
    for req in ["code", "qty"]:
        if req not in df.columns:
            missing.append(req)
    if missing:
        raise ValueError(f"–í —Å—Ç–æ–∫–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing)}")

    converted: List[Dict[str, Any]] = []
    for _, row in df.iterrows():
        item_code = _coerce_str(row.get("code"))
        if not item_code:
            continue

        qty = _coerce_int_nonneg(row.get("qty"))
        price = _coerce_float(row.get("price"))
        price_reserve = _coerce_float(row.get("price_reserve"))

        converted.append(
            {
                "branch": str(branch),
                "code": item_code,
                # "barcode": _coerce_str(row.get("barcode")),
                "price": price,
                "price_reserve": price_reserve,
                "qty": qty,
            }
        )

    logging.info(
        "–°—Ç–æ–∫ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: %s –ø–æ–∑–∏—Ü–∏–π (enterprise=%s, branch=%s)",
        len(converted),
        enterprise_code,
        branch,
    )
    await _send_downstream(converted, data_type="stock", enterprise_code=enterprise_code)
