# app/ftp_zoomagazin_data_service/ftp_zoomagazin_conv.py
# -*- coding: utf-8 -*-

import os
import re
import stat
import logging
import asyncio
from datetime import datetime
from ftplib import FTP, error_perm
from io import BytesIO

try:
    import chardet
except Exception:
    chardet = None

# =========================
# –ù–ê–°–¢–†–û–ô–ö–ò
# =========================
FTP_HOST = os.getenv("ZOOMAGAZIN_FTP_HOST", "164.92.213.254")
FTP_PORT = int(os.getenv("ZOOMAGAZIN_FTP_PORT", "21"))
FTP_USER = os.getenv("ZOOMAGAZIN_FTP_USER", "anonymous")
FTP_PASS = os.getenv("ZOOMAGAZIN_FTP_PASS", "")

# –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –≤—Ö–æ–¥—è—â–µ–π –ø–∞–ø–∫–∏ –Ω–∞ FTP
INCOMING_DIR_CANDIDATES = [
    os.getenv("ZOOMAGAZIN_INCOMING_DIR", "/tabletki-uploads"),
    "/upload",
]

# –ö–æ—Ä–µ–Ω—å FTP –Ω–∞ –§–° (–¥–ª—è –∞–Ω–æ–Ω–∏–º–Ω–æ–≥–æ vsftpd –æ–±—ã—á–Ω–æ /var/ftp)
FTP_FS_ROOT = os.getenv("ZOOMAGAZIN_FTP_FS_ROOT", "/var/ftp")

DEFAULT_FILE_TYPE = "catalog"  # –∏–ª–∏ "stock"

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# =========================
# –£–¢–ò–õ–ò–¢–´: –ø—É—Ç–∏, –∫–æ–¥–∏—Ä–æ–≤–∫–∏
# =========================
def _join_ftp(*parts: str) -> str:
    cleaned = [str(p).strip("/") for p in parts if p]
    return "/" if not cleaned else ("/" + "/".join(cleaned))

def _ftp_to_fs_path(ftp_abs: str) -> str:
    """–ú—ç–ø–ø–∏–Ω–≥ FTP-–ø—É—Ç–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –Ω–∞ –§–°: /tabletki-uploads -> /var/ftp/tabletki-uploads"""
    return os.path.join(FTP_FS_ROOT, ftp_abs.lstrip("/"))

def _decode_bytes(raw: bytes) -> str:
    for enc in ("utf-8", "utf-8-sig", "cp1251", "latin1"):
        try:
            return raw.decode(enc)
        except UnicodeDecodeError:
            pass
    if chardet:
        enc = chardet.detect(raw).get("encoding") or "utf-8"
        try:
            return raw.decode(enc, errors="replace")
        except Exception:
            pass
    return raw.decode("latin1", errors="replace")

def _normalize_dst_name(file_type: str, filename: str) -> str:
    base, _ext = os.path.splitext(filename)
    low = base.lower()
    if low.startswith("catalog-"):
        base = base[8:]
    elif low.startswith("stock-"):
        base = base[6:]
    prefix = "catalog" if file_type == "catalog" else "stock"
    return f"{prefix}-{base}.json"

# =========================
# –õ–û–ö–ê–õ–¨–ù–ê–Ø –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø (–§–ê–ô–õ–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê)
# =========================

# –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –¥–ª—è slug
TRANSLIT = {
    ord('–ê'): 'A', ord('–ë'): 'B', ord('–í'): 'V', ord('–ì'): 'G', ord('–î'): 'D',
    ord('–ï'): 'E', ord('–Å'): 'E', ord('–ñ'): 'Zh', ord('–ó'): 'Z', ord('–ò'): 'I',
    ord('–ô'): 'Y', ord('–ö'): 'K', ord('–õ'): 'L', ord('–ú'): 'M', ord('–ù'): 'N',
    ord('–û'): 'O', ord('–ü'): 'P', ord('–†'): 'R', ord('–°'): 'S', ord('–¢'): 'T',
    ord('–£'): 'U', ord('–§'): 'F', ord('–•'): 'H', ord('–¶'): 'C', ord('–ß'): 'Ch',
    ord('–®'): 'Sh', ord('–©'): 'Sch', ord('–™'): '',  ord('–´'): 'Y', ord('–¨'): '',
    ord('–≠'): 'E', ord('–Æ'): 'Yu', ord('–Ø'): 'Ya',
    ord('–∞'): 'a', ord('–±'): 'b', ord('–≤'): 'v', ord('–≥'): 'g', ord('–¥'): 'd',
    ord('–µ'): 'e', ord('—ë'): 'e', ord('–∂'): 'zh', ord('–∑'): 'z', ord('–∏'): 'i',
    ord('–π'): 'y', ord('–∫'): 'k', ord('–ª'): 'l', ord('–º'): 'm', ord('–Ω'): 'n',
    ord('–æ'): 'o', ord('–ø'): 'p', ord('—Ä'): 'r', ord('—Å'): 's', ord('—Ç'): 't',
    ord('—É'): 'u', ord('—Ñ'): 'f', ord('—Ö'): 'h', ord('—Ü'): 'c', ord('—á'): 'ch',
    ord('—à'): 'sh', ord('—â'): 'sch', ord('—ä'): '',  ord('—ã'): 'y', ord('—å'): '',
    ord('—ç'): 'e', ord('—é'): 'yu', ord('—è'): 'ya',
}
_slug_re = re.compile(r"[^A-Za-z0-9]+")

def _try_decode_name(name_bytes: bytes) -> str:
    for enc in ("utf-8", "cp1251", "latin1"):
        try:
            return name_bytes.decode(enc)
        except UnicodeDecodeError:
            continue
    return name_bytes.decode("latin1", errors="replace")

def _slugify(text: str) -> str:
    text = text.translate(TRANSLIT)
    text = _slug_re.sub("-", text).strip("-")
    text = re.sub(r"-{2,}", "-", text)
    return text or "file"

def _derive_prefix_and_base(decoded: str) -> tuple[str, str]:
    name = decoded
    if name.lower().endswith(".json"):
        name = name[:-5]
    low = name.lower()
    if low.startswith("catalog-"):
        prefix = "catalog"; name = name[8:]
    elif low.startswith("stock-"):
        prefix = "stock"; name = name[6:]
    else:
        prefix = "catalog"
    base = _slugify(name)
    return prefix, base

def _ensure_0644(full_path_bytes: bytes):
    try:
        st = os.stat(full_path_bytes)
        mode = stat.S_IMODE(st.st_mode)
        if mode != 0o644:
            os.chmod(full_path_bytes, 0o644)
            try:
                human = full_path_bytes.decode("utf-8")
            except Exception:
                human = str(full_path_bytes)
            logger.info("chmod 0644: %s", human)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Å—Ç–∞–≤–∏—Ç—å 0644: %s (%s)", full_path_bytes, e)

def _unique_ascii_target(dir_bytes: bytes, prefix: str, base: str) -> bytes:
    cand = f"{prefix}-{base}.json".encode("ascii", "ignore")
    tgt = os.path.join(dir_bytes, cand)
    if not os.path.exists(tgt):
        return tgt
    ts = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    cand2 = f"{prefix}-{base}-{ts}.json".encode("ascii", "ignore")
    return os.path.join(dir_bytes, cand2)

def _normalize_dir_fs(fs_abs: str):
    """
    –õ–æ–∫–∞–ª—å–Ω–æ (–Ω–∞ –§–°) –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤—Å–µ *.json –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –∫ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–º—É –≤–∏–¥—É:
    - –ø—Ä–∞–≤–∞ 0644,
    - –∏–º—è ASCII: catalog-/stock- + slug + .json.
    """
    dir_bytes = os.fsencode(fs_abs)  # bytes
    try:
        entries = os.listdir(dir_bytes)
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞—Ç–∞–ª–æ–≥ %s: %s", fs_abs, e)
        return

    for name_b in entries:
        full_b = os.path.join(dir_bytes, name_b)
        if not os.path.isfile(full_b):
            continue
        if not name_b.lower().endswith(b".json"):
            continue

        # –ø—Ä–∞–≤–∞
        _ensure_0644(full_b)

        # check –µ—Å–ª–∏ —É–∂–µ ASCII-–Ω–æ—Ä–º
        try:
            ascii_name = name_b.decode("ascii")
            if ascii_name.lower().endswith(".json"):
                base = ascii_name[:-5].lower()
                if base.startswith("catalog-") or base.startswith("stock-"):
                    # –≤—ã–≥–ª—è–¥–∏—Ç –æ–∫ ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏–º
                    continue
        except UnicodeDecodeError:
            pass

        decoded = _try_decode_name(name_b)
        prefix, base = _derive_prefix_and_base(decoded)
        target_b = _unique_ascii_target(dir_bytes, prefix, base)
        try:
            os.rename(full_b, target_b)
            logger.info("REN: %s -> %s", decoded, os.path.basename(target_b).decode("ascii", "ignore"))
        except Exception as e:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å '%s': %s", decoded, e)

# =========================
# –°–ò–ù–•–†–û–ù–ù–´–ï FTP –§–£–ù–ö–¶–ò–ò (–≤ to_thread)
# =========================
def _connect_sync() -> FTP:
    ftp = FTP()
    ftp.encoding = "latin1"
    ftp.connect(FTP_HOST, FTP_PORT, timeout=30)
    ftp.login(FTP_USER, FTP_PASS)
    try:
        ftp.sendcmd("OPTS UTF8 OFF")
    except Exception:
        pass
    return ftp

def _safe_cwd_sync(ftp: FTP, path: str) -> bool:
    try:
        ftp.cwd(path)
        return True
    except Exception:
        return False

def _mdtm_sync(ftp: FTP, name: str):
    try:
        resp = ftp.sendcmd(f"MDTM {name}")
        if resp.startswith("213 "):
            return datetime.strptime(resp[4:].strip(), "%Y%m%d%H%M%S")
    except Exception:
        pass
    return None

def _list_json_with_mtime_sync(ftp: FTP, incoming_abs: str):
    if not _safe_cwd_sync(ftp, incoming_abs):
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤—Ö–æ–¥—è—â—É—é –ø–∞–ø–∫—É: {incoming_abs}")
    try:
        names = ftp.nlst()
    except error_perm as e:
        if "No files found" in str(e):
            names = []
        else:
            raise
    files = [(n, _mdtm_sync(ftp, n)) for n in names if n.lower().endswith(".json")]
    files.sort(key=lambda t: (t[1] or datetime.min), reverse=True)
    return files

def _retr_text_sync(ftp: FTP, directory: str, filename: str) -> str:
    """RETR –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ current dir (+ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω–∏), –ø–æ—Å–ª–µ cwd(directory)."""
    if not _safe_cwd_sync(ftp, directory):
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –≤ {directory}")

    def _retr_rel(name: str) -> bytes:
        buf = BytesIO()
        ftp.sendcmd("TYPE I")
        ftp.retrbinary("RETR " + name, buf.write)
        return buf.getvalue()

    candidates = [filename]
    if not filename.startswith("./"):
        candidates.append("./" + filename)
    try:
        candidates.append(filename.encode("latin1", "ignore").decode("cp1251", "ignore"))
    except Exception:
        pass
    try:
        candidates.append(filename.encode("latin1", "ignore").decode("utf-8", "ignore"))
    except Exception:
        pass

    seen = set()
    candidates = [c for c in candidates if c and not (c in seen or seen.add(c))]

    last_exc = None
    for cand in candidates:
        try:
            raw = _retr_rel(cand)
            return _decode_bytes(raw)
        except Exception as e:
            last_exc = e
            continue

    raise last_exc if last_exc else error_perm("550 Failed to open file.")

def _delete_all_except_sync(ftp: FTP, directory: str, keep_name: str):
    if not _safe_cwd_sync(ftp, directory):
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–π—Ç–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ—á–∏—Å—Ç–∫–∏: %s", directory)
        return
    try:
        names = ftp.nlst()
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏: %s", e)
        return
    for n in names:
        if n == keep_name:
            continue
        if not n.lower().endswith(".json"):
            continue
        try:
            ftp.delete(n)
            logger.info("üóë –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: %s", _join_ftp(directory, n))
        except Exception as e:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å %s: %s", n, e)

def _fetch_latest_text_and_name_sync(incoming_abs: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (text, latest_name) –∏–ª–∏ (None, None).
    –ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –¥–∞—Ç–µ –∏ ¬´—Å–∫–∏–ø–∞–µ—Ç¬ª —Ç–µ, —á—Ç–æ –Ω–µ —á–∏—Ç–∞—é—Ç—Å—è (550).
    """
    ftp = None
    try:
        ftp = _connect_sync()
        files = _list_json_with_mtime_sync(ftp, incoming_abs)
        if not files:
            return (None, None)
        for name, mtime in files:
            logger.info("–ö–∞–Ω–¥–∏–¥–∞—Ç: %s (mtime=%s)", _join_ftp(incoming_abs, name), mtime or "‚Äî")
            try:
                txt = _retr_text_sync(ftp, incoming_abs, name)
                logger.info("–í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: %s", name)
                return (txt, name)
            except error_perm as e:
                if "550" in str(e):
                    logger.warning("–ü—Ä–æ–ø—É—Å–∫ (–Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ RETR): %s", name)
                    continue
                raise
            except Exception as e:
                logger.warning("–ü—Ä–æ–ø—É—Å–∫ (–æ—à–∏–±–∫–∞ RETR): %s (%s)", name, e)
                continue
        return (None, None)
    finally:
        try:
            if ftp:
                ftp.quit()
        except Exception:
            pass

def _delete_all_except_wrapper_sync(incoming_abs: str, latest_name: str):
    ftp = None
    try:
        ftp = _connect_sync()
        _delete_all_except_sync(ftp, incoming_abs, latest_name)
    finally:
        try:
            if ftp:
                ftp.quit()
        except Exception:
            pass

# =========================
# –•–£–ö–ò –û–¢–ü–†–ê–í–ö–ò –î–ê–õ–¨–®–ï (–≤–∞—à–∏ async-—Ñ—É–Ω–∫—Ü–∏–∏)
# =========================
from app.services.database_service import process_database_service  # noqa: E402

async def send_catalog_data(file_path: str, enterprise_code: int):
    await process_database_service(file_path, "catalog", enterprise_code)

async def send_stock_data(file_path: str, enterprise_code: int):
    await process_database_service(file_path, "stock", enterprise_code)

# =========================
# –û–°–ù–û–í–ù–û–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –°–¶–ï–ù–ê–†–ò–ô
# =========================
async def run_service(enterprise_code: int, file_type: str = DEFAULT_FILE_TYPE) -> bool:
    """
    –ü–æ—Ä—è–¥–æ–∫:
      0) –õ–æ–∫–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –§–° (–ø—Ä–∞–≤–∞ 0644 + ASCII-–∏–º–µ–Ω–∞) –¥–ª—è –≤—Å–µ—Ö JSON –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –≤—Ö–æ–¥—è—â–µ–π –ø–∞–ø–∫–µ.
      1) —á–µ—Ä–µ–∑ FTP –±–µ—Ä—ë–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π JSON,
      2) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ ./temp/<code>/<normalized>.json,
      3) await –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –≤–∞—à downstream (catalog/stock),
      4) —á–∏—Å—Ç–∏–º FTP: —É–¥–∞–ª—è–µ–º –≤—Å–µ .json, –∫—Ä–æ–º–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ.
    """
    try:
        # –ù–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤—Ö–æ–¥—è—â—É—é –ø–∞–ø–∫—É –Ω–∞ FTP
        incoming_abs = None
        # –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ FTP (–±—ã—Å—Ç—Ä–æ)
        def _pick_dir_sync():
            ftp = None
            try:
                ftp = _connect_sync()
                for cand in INCOMING_DIR_CANDIDATES:
                    if _safe_cwd_sync(ftp, cand):
                        return cand
                return None
            finally:
                try:
                    if ftp:
                        ftp.quit()
                except Exception:
                    pass

        incoming_abs = await asyncio.to_thread(_pick_dir_sync)
        if not incoming_abs:
            raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –≤—Ö–æ–¥—è—â–∞—è –ø–∞–ø–∫–∞ —Å—Ä–µ–¥–∏: " + ", ".join(INCOMING_DIR_CANDIDATES))

        # 0) –õ–æ–∫–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –§–°-–ø–∞–ø–∫–µ
        fs_path = _ftp_to_fs_path(incoming_abs)
        await asyncio.to_thread(_normalize_dir_fs, fs_path)

        # 1) –ü–æ–ª—É—á–∞–µ–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        text, latest_name = await asyncio.to_thread(_fetch_latest_text_and_name_sync, incoming_abs)
        if not latest_name:
            logger.info("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö JSON-—Ñ–∞–π–ª–æ–≤ (–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏).")
            return True

        # 2) –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        temp_dir = os.path.join(".", "temp", str(enterprise_code))
        os.makedirs(temp_dir, exist_ok=True)
        out_name = _normalize_dst_name(file_type, latest_name)
        out_path = os.path.join(temp_dir, out_name)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(text)
        logger.info("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: %s", out_path)

        # 3) –ü–µ—Ä–µ–¥–∞—á–∞ –¥–∞–ª—å—à–µ
        if file_type == "catalog":
            logger.info("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ catalog –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è %s", enterprise_code)
            await send_catalog_data(out_path, enterprise_code)
        else:
            logger.info("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ stock –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è %s", enterprise_code)
            await send_stock_data(out_path, enterprise_code)
        logger.info("–î–∞–Ω–Ω—ã–µ %s —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è %s", file_type, enterprise_code)

        # 4) –û—á–∏—Å—Ç–∫–∞: –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
        await asyncio.to_thread(_delete_all_except_wrapper_sync, incoming_abs, latest_name)
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–æ—Å—Ç–∞–≤–ª–µ–Ω —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª).")

        return True

    except Exception as e:
        logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ run_service: %s", e, exc_info=True)
        return False

# =========================
# CLI (—Ä—É—á–Ω–æ–π —Ç–µ—Å—Ç)
# =========================
if __name__ == "__main__":
    import argparse

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    parser = argparse.ArgumentParser(description="FtpZoomagazin processor (async with FS normalization)")
    parser.add_argument("--enterprise", type=int, required=True, help="enterprise_code")
    parser.add_argument("--type", choices=["catalog", "stock"], default=DEFAULT_FILE_TYPE, help="file type to process")
    args = parser.parse_args()

    async def _amain():
        ok = await run_service(args.enterprise, args.type)
        raise SystemExit(0 if ok else 1)

    asyncio.run(_amain())
