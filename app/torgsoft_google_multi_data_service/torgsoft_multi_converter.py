# app/torgsoft_google/torgsoft_converter.py

import asyncio
import json
import logging
import math
import os
import tempfile
from typing import Any, Dict, List, Optional

import pandas as pd

from sqlalchemy.future import select
from app.database import get_async_db
from app.database import get_async_db, MappingBranch

async def fetch_branch(enterprise_code: str, store_id: str) -> str:
    async with get_async_db() as session:
        result = await session.execute(
            select(MappingBranch.branch).where(
                MappingBranch.enterprise_code == enterprise_code,
                MappingBranch.store_id == store_id
            )
        )
        branch = result.scalars().first()
        if not branch:
            raise ValueError(f"‚ùå Branch –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è enterprise_code={enterprise_code}, store_id={store_id}")
        return str(branch)


# üëâ –ó–∞–º–µ–Ω–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—É—Ç—å –≤ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ, –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
try:
    from app.services.database_service import process_database_service  # type: ignore
except Exception:
    try:
        from app.services.stock_scheduler_service import process_database_service  # type: ignore
    except Exception:
        process_database_service = None  # –±—É–¥–µ—Ç –ª–æ–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON-—Ñ–∞–π–ª–∞ –±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# =========================
# –í–ù–£–¢–†–ï–ù–ù–ò–ï –£–¢–ò–õ–ò–¢–´
# =========================

UA_TO_STD_MAP = {
    # –û–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–¥–∞:
    "–∫–æ–¥ —Ñ–æ—Ç–æ": "code",

    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
    "–Ω–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É": "name",
    "–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—É": "name",

    # –®—Ç—Ä–∏—Ö–∫–æ–¥
    "—à—Ç—Ä–∏—Ö-–∫–æ–¥": "barcode",
    "—à—Ç—Ä–∏—Ö–∫–æ–¥": "barcode",

    # –¶–µ–Ω—ã (–¥–ª—è —Å—Ç–æ–∫–∞)
    "—Ü—ñ–Ω–∞ —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞": "price",
    "—Ü–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è": "price",
    "—Ü—ñ–Ω–∞ –∑—ñ –∑–Ω–∏–∂–∫–æ—é": "price_reserve",
    "—Ü–µ–Ω–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π": "price_reserve",

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ (–¥–ª—è —Å—Ç–æ–∫–∞)
    "–∫—ñ–ª—å–∫—ñ—Å—Ç—å": "qty",
    "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ": "qty",

    # –ë–æ–ª—å—à–µ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º "‚Ññ" –∫–∞–∫ –∫–æ–¥ ‚Äî –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–æ
    # "‚Ññ": "code",
    # "–Ω–æ–º–µ—Ä": "code",
    # "‚Ññ –∑/–ø": "code",
}

DEFAULT_PRODUCER = "N/A"
DEFAULT_VAT = 20.0


def _get_temp_dir() -> str:
    tmp = os.getenv("TEMP_DIR", tempfile.gettempdir())
    os.makedirs(tmp, exist_ok=True)
    return tmp


def _norm(s: Any) -> str:
    return str(s).strip().lower() if s is not None else ""


def _coerce_str(v: Any) -> str:
    if v is None or (isinstance(v, float) and math.isnan(v)):
        return ""
    s = str(v).strip()
    # –°–æ—Ö—Ä–∞–Ω–∏–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏ –¥–ª—è —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–≤
    return s


def _coerce_float(v: Any, default: float = 0.0) -> float:
    try:
        if v is None or (isinstance(v, float) and math.isnan(v)):
            return default
        s = str(v).replace(",", ".").strip()
        return float(s) if s else default
    except Exception:
        return default


def _coerce_int_nonneg(v: Any, default: int = 0) -> int:
    try:
        if v is None or (isinstance(v, float) and math.isnan(v)):
            return default
        if isinstance(v, str) and not v.strip():
            return default
        val = int(float(str(v).replace(",", ".").strip()))
        return max(val, 0)
    except Exception:
        return default


def _read_table(file_path: str) -> pd.DataFrame:
    """
    –ß–∏—Ç–∞–µ—Ç Excel/CSV –∏ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.
    –†–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ 1‚Äì10 —Å—Ç—Ä–æ–∫–µ (–≤–∫–ª—é—á–∞—è 3-—é —Å—Ç—Ä–æ–∫—É).
    """
    def _normalize_cols(cols):
        out = []
        for c in cols:
            s = _norm(c)
            out.append(" ".join(s.split()))
        return out

    def _detect_header_row(df_probe: pd.DataFrame) -> int:
        # –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —É–∑–Ω–∞—ë–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header_hints = [
            "–∫–æ–¥ —Ñ–æ—Ç–æ",           # üëà –¥–æ–±–∞–≤–∏–ª–∏
            "–Ω–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É", "–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—É",
            "—à—Ç—Ä–∏—Ö-–∫–æ–¥", "—à—Ç—Ä–∏—Ö–∫–æ–¥",
            "–∫—ñ–ª—å–∫—ñ—Å—Ç—å", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "—Ü—ñ–Ω–∞ —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞", "—Ü–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è",
            "—Ü—ñ–Ω–∞ –∑—ñ –∑–Ω–∏–∂–∫–æ—é", "—Ü–µ–Ω–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π",
            # (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å "‚Ññ" –≤ —Ö–∏–Ω—Ç–∞—Ö –∫–∞–∫ –≤—Ç–æ—Ä–∏—á–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞,
            # —ç—Ç–æ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è code)
            "‚Ññ", "–Ω–æ–º–µ—Ä", "‚Ññ –∑/–ø",
        ]
        max_rows_to_check = min(len(df_probe), 10)
        for i in range(max_rows_to_check):
            row_vals = _normalize_cols(list(df_probe.iloc[i].values))
            joined = " ".join(row_vals)
            # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤ —Å—Ç—Ä–æ–∫–µ –¥–æ–ª–∂–Ω—ã –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è —Ö–æ—Ç—è –±—ã 2 –ø–æ–¥—Å–∫–∞–∑–∫–∏
            hits = sum(1 for h in header_hints if h in joined)
            if hits >= 2:
                return i
        # –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
        return 0

    # --- —á–∏—Ç–∞–µ–º "—á–µ—Ä–Ω–æ–≤–∏–∫" –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —à–∞–ø–∫–∏
    if file_path.lower().endswith((".xlsx", ".xls")):
        probe = pd.read_excel(file_path, header=None, nrows=10, dtype=object)
        header_row = _detect_header_row(probe)
        df = pd.read_excel(file_path, header=header_row, dtype=object)
    elif file_path.lower().endswith(".csv"):
        try:
            probe = pd.read_csv(file_path, header=None, nrows=10, dtype=object)
        except UnicodeDecodeError:
            probe = pd.read_csv(file_path, header=None, nrows=10, dtype=object, encoding="cp1251")
        header_row = _detect_header_row(probe)
        try:
            df = pd.read_csv(file_path, header=header_row, dtype=object)
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, header=header_row, dtype=object, encoding="cp1251")
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
    df.columns = [" ".join(_norm(c).split()) for c in df.columns]

    # –ª–æ–≥ –¥–ª—è –¥–µ–±–∞–≥–∞ ‚Äî —É–≤–∏–¥–∏–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏—Å—å
    logging.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: %s", list(df.columns))
    return df



def _map_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç–æ–ª–±—Ü—ã –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –∏–º–µ–Ω–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ UA_TO_STD_MAP.
    –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å.
    """
    rename_map: Dict[str, str] = {}
    for col in df.columns:
        std = UA_TO_STD_MAP.get(col, None)
        if std:
            rename_map[col] = std
    if rename_map:
        df = df.rename(columns=rename_map)
    return df


def _save_json_temp(data: List[Dict[str, Any]], prefix: str) -> str:
    out_path = os.path.join(_get_temp_dir(), f"{prefix}_torgsoft_{next(tempfile._get_candidate_names())}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return out_path


async def _send_downstream(data: List[Dict[str, Any]], data_type: str, enterprise_code: str) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–ª—å—à–µ –ø–æ –ø—Ä–æ–µ–∫—Ç—É:
    - –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω process_database_service -> –≤—ã–∑—ã–≤–∞–µ–º –µ–≥–æ.
    - –ò–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –ø–∏—à–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ –ª–æ–≥.
    """
    if process_database_service is None:
        path = _save_json_temp(data, prefix=data_type)
        logging.warning(
            "process_database_service –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π JSON: %s", path
        )
        return

    # –°–µ—Ä–≤–∏—Å –æ–∂–∏–¥–∞–µ—Ç –ø—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É
    path = _save_json_temp(data, prefix=data_type)
    logging.info("–û—Ç–ø—Ä–∞–≤–∫–∞ %s –¥–∞–Ω–Ω—ã—Ö –≤ process_database_service: %s", data_type, path)
    try:
        if asyncio.iscoroutinefunction(process_database_service):
            await process_database_service(path, data_type, enterprise_code)
        else:
            # –ù–∞ —Å–ª—É—á–∞–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, process_database_service, path, data_type, enterprise_code)
    finally:
        try:
            os.remove(path)
        except Exception:
            pass


# =========================
# –ü–£–ë–õ–ò–ß–ù–´–ï –§–£–ù–ö–¶–ò–ò –ö–û–ù–í–ï–†–¢–ê–¶–ò–ò
# =========================

async def process_torgsoft_catalog(
    enterprise_code: str,
    file_path: str,
    file_type: str = "catalog",
) -> None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∞ ¬´–°—Ç–∞–Ω —Å–∫–ª–∞–¥¬ª.
    –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –¥—É–±–ª–∏ –ø–æ code (–ö–æ–¥ —Ñ–æ—Ç–æ) ‚Äî –≤ –∏—Ç–æ–≥–æ–≤–æ–º —Ñ–∞–π–ª–µ
    –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∞.
    """
    df = _read_table(file_path)
    df = _map_columns(df)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    missing = [c for c in ("code", "name") if c not in df.columns]
    if missing:
        raise ValueError(f"–í –∫–∞—Ç–∞–ª–æ–≥–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing)}")

    items_by_code: Dict[str, Dict[str, Any]] = {}

    for _, row in df.iterrows():
        item_code = _coerce_str(row.get("code"))
        item_name = _coerce_str(row.get("name"))

        if not item_code or not item_name:
            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏

        # –ö–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –¥—É–±–ª—å –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–π
        items_by_code[item_code] = {
            "code": item_code,
            "name": item_name,
            "producer": DEFAULT_PRODUCER,
            "vat": DEFAULT_VAT,
            "barcode": _coerce_str(row.get("barcode")),
        }

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫
    converted: List[Dict[str, Any]] = list(items_by_code.values())

    logging.info(
        "–ö–∞—Ç–∞–ª–æ–≥ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: %s –ø–æ–∑–∏—Ü–∏–π (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã, –¥—É–±–ª–∏ —É–¥–∞–ª–µ–Ω—ã)",
        len(converted),
    )
    await _send_downstream(converted, data_type="catalog", enterprise_code=enterprise_code)




from sqlalchemy.future import select
from app.database import get_async_db, MappingBranch

def _norm_store_id(s: str) -> str:
    s = (s or "").strip().lower()
    # —É–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –¥–µ—Ñ–∏—Å—ã –∏ –ø—Ä–æ–±–µ–ª—ã
    s = s.replace("‚Äî", "-").replace("‚Äì", "-")
    s = " ".join(s.split())           # —Å–∂–∞—Ç—å –∫—Ä–∞—Ç–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = s.replace(" - ", "-").replace(" -", "-").replace("- ", "-")
    return s

async def _build_branch_map(enterprise_code: str) -> dict[str, str]:
    """–ß–∏—Ç–∞–µ–º –≤—Å–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–∫–ª–∞–¥->branch –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ enterprise_code –∏ –∫—ç—à–∏—Ä—É–µ–º."""
    async with get_async_db() as session:
        result = await session.execute(
            select(MappingBranch.store_id, MappingBranch.branch)
            .where(MappingBranch.enterprise_code == enterprise_code)
        )
        mapping = {}
        for store_id, branch in result.all():
            key = _norm_store_id(str(store_id))
            if key:
                mapping[key] = str(branch)
        return mapping

async def process_torgsoft_stock(
    enterprise_code: str,
    file_path: str,
    file_type: str = "stock",
) -> None:
    """
    –°—Ç–æ–∫ –∏–∑ ¬´–°—Ç–∞–Ω.xlsx/18.xlsx¬ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–∫–ª–∞–¥–∞–º–∏.
    branch –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –ø–æ enterprise_code + store_id (–∫–æ–ª–æ–Ω–∫–∞ ¬´–°–∫–ª–∞–¥¬ª).
    """
    df = _read_table(file_path)
    df = _map_columns(df)

    # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
    required = ["code", "qty", "—Å–∫–ª–∞–¥"]
    miss = [c for c in required if c not in df.columns]
    if miss:
        raise ValueError(f"–í —Å—Ç–æ–∫–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(miss)}")

    # –ö—ç—à —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Å–∫–ª–∞–¥->branch
    store_to_branch = await _build_branch_map(enterprise_code)

    converted = []
    skipped_rows = 0

    for _, row in df.iterrows():
        item_code = _coerce_str(row.get("code"))
        store_id_raw = _coerce_str(row.get("—Å–∫–ª–∞–¥"))

        # 1) –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö/—Å–ª—É–∂–µ–±–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –∏ ¬´–≤—Ç–æ—Ä—ã—Ö —à–∞–ø–æ–∫¬ª
        if not item_code or not store_id_raw:
            skipped_rows += 1
            continue
        if item_code.lower() == "–∫–æ–¥ —Ñ–æ—Ç–æ" or store_id_raw.lower() == "—Å–∫–ª–∞–¥":
            skipped_rows += 1
            continue

        qty = _coerce_int_nonneg(row.get("qty"))
        price = _coerce_float(row.get("price"))
        price_reserve = _coerce_float(row.get("price_reserve"))

        key = _norm_store_id(store_id_raw)
        branch = store_to_branch.get(key)

        if not branch:
            # –î–æ–ø. –ø–æ–ø—ã—Ç–∫–∞: –∏–Ω–æ–≥–¥–∞ –≤ —Ñ–∞–π–ª–µ –º–æ–≥—É—Ç –±—ã—Ç—å ¬´30421¬ª ‚Äî –ø—Ä—è–º—ã–µ –∫–æ–¥—ã —Å–∫–ª–∞–¥–∞
            branch = store_to_branch.get(_norm_store_id(str(int(float(store_id_raw)))) if store_id_raw.replace(".", "", 1).isdigit() else "")
        if not branch:
            raise ValueError(f"‚ùå Branch –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è enterprise_code={enterprise_code}, store_id={store_id_raw}")

        converted.append({
            "branch": branch,
            "code": item_code,
            "price": price,
            "price_reserve": price_reserve,
            "qty": qty,
        })

    logging.info(
        "–°—Ç–æ–∫ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: %s –ø–æ–∑–∏—Ü–∏–π (enterprise=%s). –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç—Ä–æ–∫: %s",
        len(converted), enterprise_code, skipped_rows
    )
    await _send_downstream(converted, data_type="stock", enterprise_code=enterprise_code)
